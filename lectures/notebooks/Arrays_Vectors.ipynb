{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrays and Vectors\n",
    "\n",
    "Arrays are far more general objects than vectors or matrices, yet arrays can be (and are) used to represent vectors and matrices.  Here will explore some of the implications of this and some of the care that must be taken when using a general object (array) to represent specific objects (vectors and matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.linalg as la\n",
    "# Globally fix plot styling\n",
    "import matplotlib as mpl\n",
    "mpl.rc('xtick', direction='in', top=True)\n",
    "mpl.rc('ytick', direction='in', right=True)\n",
    "mpl.rc('xtick.minor', visible=True)\n",
    "mpl.rc('ytick.minor', visible=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief aside on documentation\n",
    "\n",
    "One way to look up documentation on different functions is to append, or pre-pend a question mark. For example, we can access documentation on the `np.min` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many commonly used modules in Python have excellent documentation. However, be careful! Community-contributed modules may not be well-documented, and may have inconsistent conventions. One example I have encountered recently is the `pywigxjpf` module. This module is useful for computing Wigner symbols and Clebschâ€“Gordan coefficients. The source version, Conda version, and PyPI version all accepted similar but slightly different argument formats; and were not documented.\n",
    "\n",
    "You can define your own documentation too! If I define a function, I can similarly access its documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysquare(x) :\n",
    "    \"\"\"\n",
    "    Return the square of the argument.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Input value.\n",
    "    \n",
    "    out : value, squared.\n",
    "    \"\"\"\n",
    "    return x**2\n",
    "\n",
    "mysquare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists versus Arrays\n",
    "\n",
    "The basic data structures in Python itself are lists and tuples.  We can think of a tuple as a read only list.  The basic data structure in NumPy is an array.  Arrays and lists are not the same thing, even though most NumPy and SciPy functions will treat them as the same.  In fact, many of us have \"gotten away with\" using a list where we meant an array, since the functions we used silently converted lists to arrays.\n",
    "\n",
    "To see that there is a difference we first construct the two different types of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List : [1.0, 2, 3]\n",
      "Array: [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "a_list = [ 1., 2, 3 ]\n",
    "# We can convert the list to an array\n",
    "a_arr = np.array(a_list)\n",
    "print(f\"\"\"List : {a_list}\n",
    "Array: {a_arr}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these are different!  The list preserves the types of the objects we put into it.  The first element is a float and the last two are integers.  The array, on the other hand, converts everything to the same (largest) type, so everything is a float.  Python lists can contain different types of elements.  NumPy arrays must have all the same type.\n",
    "\n",
    "Next, if we try to add a float to a list or an array the addition works fine for the array but gives an error for the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array sum : [2.1 3.1 4.1]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"float\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-97e66b9d44ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Array sum :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_arr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"List sum  :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_list\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"float\") to list"
     ]
    }
   ],
   "source": [
    "print(\"Array sum :\", a_arr+1.1)\n",
    "print(\"List sum  :\", a_list+1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it did try to interpret the addition in some way, it just could not make sense of the operation.\n",
    "\n",
    "On the other hand, if we multiplied by an integer ...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array mult : [2. 4. 6.]\n",
      "List mult  : [1.0, 2, 3, 1.0, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Array mult :\", a_arr*2)\n",
    "print(\"List mult  :\", a_list*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary of this is that we must be careful with how we represent our information.  If we mean to use arrays then we should use arrays.  Even if the functions we are using silently \"fix\" this for us, we always should use the correct data structure.  If we do not, then we will run into a problem, eventually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "Another extremely powerful feature of arrays is broadcasting.  When dealing with matrices and vectors an expression such as $\\mathsf{A} + \\vec v$ makes no sense.  It is complete nonsense!  However, when dealing with arrays we can define what this means.  Similarly, we know that the multiplication of a matrix and a vector, $\\mathsf{A}\\vec v$, does make sense and produces a vector.  So what does the multiplication of a two dimensional array and a one dimensional array produce?\n",
    "\n",
    "The answer to all questions relating to the combination of different dimensional arrays is broadcasting.  There are many powerful things that can be done with broadcasting, here we will just understand the basics.  Though we will list some rules, the best approach, as always, is to **test your operations**.  There is no need to guess at how things will behave, we can easily check!\n",
    "\n",
    "**Basic Rule**\n",
    "\n",
    "For array `A` with one more dimension than array `v`, broadcasting will \"repeat\" `v` along the missing dimensions of `A`.  For the two dimensional case this means if `A` is a $M\\times N$ array, then `v` must be of length $N$ and broadcasting will \"repeat\" it $M$ times and operate on each row of `A` using the same `v`.\n",
    "\n",
    "This can be phrased in more sophisticated ways, but it is easiest to see in an example.  Here we create an array with 3 columns (notice the use of `reshape`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [[0 1 2]\n",
      " [3 4 5]]\n",
      "v = [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "A = np.arange(6).reshape((2,3))\n",
    "v = np.arange(3) + 1\n",
    "print(\"A =\", A)\n",
    "print(\"v =\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add and multiply these two.  Again notice that the operations are performed on each component of `A` using the same components of `v` for each row.  More explicitly, for the addition case\n",
    "$$ (A+v)_{ij} = A_{ij} + v_j. $$\n",
    "Similarly for multiplication (and it would also be the same for subtraction, division, and other operators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A+v = [[1 3 5]\n",
      " [4 6 8]]\n",
      "A*v = [[ 0  2  6]\n",
      " [ 3  8 15]]\n"
     ]
    }
   ],
   "source": [
    "print(\"A+v =\", A+v)\n",
    "print(\"A*v =\", A*v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to broadcasting is that the shape of `v` must be the same shape as `A`, in all of its final dimensions.  In the two dimensional case this means if `A` is $N\\times M$ and `v` is of length $N$ then **broadcasting will not work** (at least not for $N\\ne M$).\n",
    "\n",
    "We can see that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "v = [1 2 3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-eaca4f7b0520>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"A =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"v =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"A+v =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2) (3,) "
     ]
    }
   ],
   "source": [
    "A = np.arange(6).reshape((3,2))\n",
    "v = np.arange(3) + 1\n",
    "print(\"A =\", A)\n",
    "print(\"v =\", v)\n",
    "print(\"A+v =\", A+v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the error even tells us the broadcasting fails, and why.  How can we fix this?  Well, what we wanted to happen was to have `v` added to the *columns* of `A`, not the rows.  This can easily be accomplished by taking the transpose of `A`.  Since taking the transpose is so common there is shorthand for doing this, `A.T`.  Here, if we take the transpose of `A` we can add it to `v` using the usual broadcasting.  This is great, except the answer we get will have the rows and columns swapped.  How do we fix this?  Just take the transpose of the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected A+v = [[1 2]\n",
      " [4 5]\n",
      " [7 8]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Corrected A+v =\", (A.T + v).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Technical details:* The way this, and most slicing, is done is using what is called *strides*.  It really just changes the way the array is accessed.  No changes are made to the actual order of data in the array.  It is not necessary to know this to use broadcasting, but it does mean that it not computationally expensive to use.  It is implemented in an efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix operations\n",
    "\n",
    "We have some familiarity with arrays since we have already been using them.  For linear algebra, which we will discuss further later in the class, the main routines are contained in `scipy.linalg`.  Behind the scenes many calculations are done using `BLAS` and `LAPACK` libraries.  Depending on how your version was compiled these can be highly optimized and even parallelized.\n",
    "\n",
    "*Notes:*\n",
    "1. Some functions *also* have implementations in NumPy.  In general we are going to prefer the versions from `scipy.linalg`.  In practice, both versions should give the same results except in extreme cases, such as dealing with (numerically) singular matrices.\n",
    "2. Since we will represent both vectors and matrices using NumPy arrays we will need to be careful.  For example, we know what multiplying a matrix and a vector means, whereas multiplying a two dimensional array and a one dimensional array is not uniquely defined.  There *is* a matrix object in NumPy but it is not well developed nor well supported.  In fact, it is deprecated and will be removed from future versions.  It should never be used!  Due to the fact that we are using arrays instead of matrices this means we will need to use a special function, `np.dot`, or a special operator, `@`, when we multiply a matrix and a vector. (We will discuss the `np.dot` function more below and the `@` operator in the future. The distinction between an array and a vector is an important detail!)\n",
    "\n",
    "As always, begin by looking at the documentation.  From `scipy.linalg` we see there are many routines.  Here we will focus on some of those from \"Basics\".  We will encounter some of the other functions in future weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "la?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual there are many things defined in this module.  For our purposes here we are interested in the basic functionality, in particular calculating inverses using `la.inv()`.  There are many other specialized functions, many matrix decompositions, *etc*., all of which have their particular uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "la.inv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this calculates the inverse for a **square matrix**.  There are options for optimizing the function that are common to many of the functions in `scipy.linalg`.  These are particularly useful when dealing with large matrices when we want to avoid making copies.  By default these functions do make a copy and do not overwrite their inputs.  This is a good default, but can lead to slower, more memory intensive code.  For all of our uses this will not be important so we will just use the defaults.\n",
    "\n",
    "As one test we can verify that `la.inv()` really does calculate an inverse.  Recall that for a square matrix with $\\det(\\mathsf{A})\\ne 0$ there exists an inverse, $\\mathsf{A}^{-1}$ that satisfies\n",
    "$$ \\mathsf{A} \\mathsf{A}^{-1} = \\mathsf{A}^{-1} \\mathsf{A} = \\mathsf{1}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "det(A) =\n",
      " 0.14320419636627074\n",
      "A Ainv =\n",
      " [[ 1.00000000e+00  1.30204909e-16 -1.79675558e-16]\n",
      " [ 1.08928249e-16  1.00000000e+00 -1.88761513e-16]\n",
      " [ 1.18775378e-16  2.96863822e-16  1.00000000e+00]]\n",
      "Ainv A =\n",
      " [[ 1.00000000e+00 -3.85441056e-16  2.49385471e-16]\n",
      " [-7.10269450e-17  1.00000000e+00  1.67169121e-16]\n",
      " [ 7.40317575e-17  1.27822265e-16  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Construct a random matrix\n",
    "A = np.random.random(size=(3,3))\n",
    "# Calculate its inverse\n",
    "Ainv = la.inv(A)\n",
    "# Print some results\n",
    "print(f\"\"\"\n",
    "det(A) =\\n {la.det(A)}\n",
    "A Ainv =\\n {np.dot(A, Ainv)}\n",
    "Ainv A =\\n {np.dot(Ainv, A)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few things to notice here.  First, we used `np.dot()` to multiply the matrices, we **did not use** `A*Ainv`.  Why not?\n",
    "\n",
    "Let us compare the two options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "np.dot(A,Ainv) =\n",
      " [[ 1.00000000e+00  1.30204909e-16 -1.79675558e-16]\n",
      " [ 1.08928249e-16  1.00000000e+00 -1.88761513e-16]\n",
      " [ 1.18775378e-16  2.96863822e-16  1.00000000e+00]]\n",
      "A*Ainv =\n",
      " [[-0.30085366  0.72776594  1.36432709]\n",
      " [ 0.17336493 -0.45239308  0.85824376]\n",
      " [ 0.65300737  1.39831779 -1.04027362]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "np.dot(A,Ainv) =\\n {np.dot(A,Ainv)}\n",
    "A*Ainv =\\n {A*Ainv}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first case gives the identity matrix (or something close to it) but what do we get in the second case?  The function `np.dot()` does *matrix multiplication* :\n",
    "$$ (\\mathsf{A} \\mathsf{A}^{-1})_{ij} = \\sum_{k} A_{ik} (A^{-1})_{kj}. $$\n",
    "\n",
    "The usual multiplication, `A*Ainv`, multiplies the components of the two arrays\n",
    "$$ (\\mathsf{A} * \\mathsf{A}^{-1})_{ij} = A_{ij} * (A^{-1})_{ij}. $$\n",
    "\n",
    "In both cases we end up with a two dimensional array, but they are very different arrays!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to notice is that we are suppose to get the identity matrix.  We can construct this in NumPy in a few ways, one is using `np.eye()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(A.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your matrix multiplications actually return the identity matrix?  We see that it is not exactly the identity matrix, but we do not expect it to be.  (Why not?)  For small arrays like we have here we can look at all the components and see they the diagonal entries are one and the off diagonal entries are close enough to zero, as expected.  For larger arrays we can once again use `np.allclose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.allclose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the documentation describes, this will test each component of our arrays testing if they are \"close\" to each other in both an absolute and relative sense.  In other words, there are two tolerances.  The formula described in the documentation is, for components $a$ and $b$ of the two arrays, respectively, we say these values are close if\n",
    "$$ |a - b| \\le (a_{\\mathrm{tol}} + r_{\\mathrm{tol}} |b|), $$\n",
    "where $a_{\\mathrm{tol}}$ is the absolute tolerance and $r_{\\mathrm{tol}}$ is the relative tolerance.  As the documentation also notes, this definition is asymmetric, $a$ and $b$ are treated differently.  This is an unfortunate choice and something that may be corrected in the future.\n",
    "\n",
    "For our purposes we can use this to verify the inverse found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Ainv = 1? True\n",
      "Ainv A = 1? True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "A Ainv = 1? {np.allclose(np.eye(A.shape[0]),np.dot(A, Ainv))}\n",
    "Ainv A = 1? {np.allclose(np.eye(A.shape[0]),np.dot(Ainv, A))}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
